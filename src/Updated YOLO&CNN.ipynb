{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1N0MLMa-qMiJza5gWSVfN_Jy3kh5Is1yP","timestamp":1732559994470}],"gpuType":"T4","authorship_tag":"ABX9TyMrIKQw9RqvrWro6k0YfB3a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpCrsiKRVKte","executionInfo":{"status":"ok","timestamp":1732676165860,"user_tz":300,"elapsed":33169,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"6c12f105-6b0f-4e5a-96b6-5d68d11cf459"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install roboflow supervision opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxkDR5Y7Vq8G","executionInfo":{"status":"ok","timestamp":1732676211295,"user_tz":300,"elapsed":4338,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"64011eb3-fd8e-451d-a194-62bec20b6156","collapsed":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting roboflow\n","  Downloading roboflow-1.1.49-py3-none-any.whl.metadata (9.7 kB)\n","Collecting supervision\n","  Downloading supervision-0.25.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n","Collecting idna==3.7 (from roboflow)\n","  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.8.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n","Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.6)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n","Collecting filetype (from roboflow)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.3.1)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n","Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.13.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.55.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n","Downloading roboflow-1.1.49-py3-none-any.whl (80 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading supervision-0.25.0-py3-none-any.whl (181 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: filetype, python-dotenv, idna, supervision, roboflow\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 roboflow-1.1.49 supervision-0.25.0\n"]}]},{"cell_type":"code","source":["import torch\n","from roboflow import Roboflow\n","import supervision as sv\n","import cv2\n","import os\n","from PIL import Image"],"metadata":{"id":"hxV9kSTJVvdo","executionInfo":{"status":"ok","timestamp":1732676214113,"user_tz":300,"elapsed":1499,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# rf = Roboflow(api_key=\"ayL48plh8fqJBfMTdBUb\")\n","# # project = rf.workspace(\"steph-r3xmc\").project(\"fridgevision-ms4yf\")\n","# # version = project.version(2)\n","# # dataset = version.download(\"yolov5\")\n","# project = rf.workspace(\"steph-r3xmc\").project(\"oneclassdata\")\n","# version = project.version(1)\n","# dataset = version.download(\"yolov5\")\n","\n","\n","\n","# new data after tiff\n","\n","rf = Roboflow(api_key=\"Hxf0mSTlVAZoc1jBBQJr\")\n","project = rf.workspace(\"tiffanyzha\").project(\"oneclassfridgedata\")\n","version = project.version(1)\n","dataset = version.download(\"yolov5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2xSsbNxV-ma","executionInfo":{"status":"ok","timestamp":1732564043093,"user_tz":300,"elapsed":23822,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"d3daaffa-7e64-409d-96bd-eb77a56eaf07","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in OneClassFridgeData-1 to yolov5pytorch:: 100%|██████████| 266177/266177 [00:16<00:00, 16086.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to OneClassFridgeData-1 in yolov5pytorch:: 100%|██████████| 11394/11394 [00:01<00:00, 5951.44it/s]\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/ultralytics/yolov5\n","%cd yolov5\n","!pip install -r requirements.txt\n","from yolov5.models.yolo import Model\n","from yolov5.utils.dataloaders import LoadImagesAndLabels\n","from yolov5.utils.general import check_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8bTvza9V2OW","executionInfo":{"status":"ok","timestamp":1732676234957,"user_tz":300,"elapsed":15318,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"09568a48-1cad-4cc3-9778-b59a42b84bdf","collapsed":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 17067, done.\u001b[K\n","remote: Counting objects: 100% (45/45), done.\u001b[K\n","remote: Compressing objects: 100% (37/37), done.\u001b[K\n","remote: Total 17067 (delta 22), reused 23 (delta 8), pack-reused 17022 (from 1)\u001b[K\n","Receiving objects: 100% (17067/17067), 15.69 MiB | 12.90 MiB/s, done.\n","Resolving deltas: 100% (11716/11716), done.\n","/content/yolov5\n","Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.43)\n","Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\n","Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (11.0.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n","Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.20.1+cu121)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.6)\n","Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n","  Downloading ultralytics-8.3.38-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n","Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.11)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n","  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n","Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Downloading ultralytics-8.3.38-py3-none-any.whl (896 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.3/896.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, thop, ultralytics\n","Successfully installed thop-0.1.1.post2209072238 ultralytics-8.3.38 ultralytics-thop-2.0.12\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}]},{"cell_type":"code","source":["!wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n","!mv yolov5s.pt /content/yolov5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90LZtQVgV2vv","executionInfo":{"status":"ok","timestamp":1732676243136,"user_tz":300,"elapsed":2823,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"5600c098-627c-46d5-e0b4-36d2f26b6d14","collapsed":true},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-11-27 02:57:20--  https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n","Resolving github.com (github.com)... 20.27.177.113\n","Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/eab38592-7168-4731-bdff-ad5ede2002be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241127T025720Z&X-Amz-Expires=300&X-Amz-Signature=7abb72ebd9ef615a4842c8848097eac783937b04c623bc7adddd6c942459bde1&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n","--2024-11-27 02:57:20--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/eab38592-7168-4731-bdff-ad5ede2002be?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241127T025720Z&X-Amz-Expires=300&X-Amz-Signature=7abb72ebd9ef615a4842c8848097eac783937b04c623bc7adddd6c942459bde1&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14698491 (14M) [application/octet-stream]\n","Saving to: ‘yolov5s.pt’\n","\n","yolov5s.pt          100%[===================>]  14.02M  20.0MB/s    in 0.7s    \n","\n","2024-11-27 02:57:22 (20.0 MB/s) - ‘yolov5s.pt’ saved [14698491/14698491]\n","\n","mv: 'yolov5s.pt' and '/content/yolov5/yolov5s.pt' are the same file\n"]}]},{"cell_type":"code","source":["!yolo task=detect mode=train model=/content/yolov5/yolov5s.pt data=/content/OneClassFridgeData-1/data.yaml epochs=10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-7OxIJ8mOYQ","executionInfo":{"status":"ok","timestamp":1732565531556,"user_tz":300,"elapsed":1122173,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"60d8abb1-08b6-4ceb-cd9e-fdac93797c82","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PRO TIP 💡 Replace 'model=/content/yolov5/yolov5s.pt' with new 'model=/content/yolov5/yolov5su.pt'.\n","YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n","\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt to '/content/yolov5/yolov5su.pt'...\n","100% 17.7M/17.7M [00:00<00:00, 32.2MB/s]\n","Ultralytics 8.3.37 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/yolov5/yolov5s.pt, data=/content/OneClassFridgeData-1/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","100% 755k/755k [00:00<00:00, 111MB/s]\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n","  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n","  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n","  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n","  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n"," 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n"," 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n"," 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n"," 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n"," 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n","YOLOv5s summary: 262 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n","\n","Transferred 421/427 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n","Freezing layer 'model.24.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n","100% 5.35M/5.35M [00:00<00:00, 343MB/s]\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/OneClassFridgeData-1/train/labels... 4657 images, 0 backgrounds, 0 corrupt: 100% 4657/4657 [00:02<00:00, 1846.81it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/OneClassFridgeData-1/train/labels.cache\n","WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 18, len(boxes) = 32377. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/OneClassFridgeData-1/valid/labels... 688 images, 0 backgrounds, 0 corrupt: 100% 688/688 [00:00<00:00, 1133.26it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/OneClassFridgeData-1/valid/labels.cache\n","Plotting labels to runs/detect/train/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/train\u001b[0m\n","Starting training for 10 epochs...\n","Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       1/10      4.36G      1.047     0.8302      1.089          7        640: 100% 292/292 [01:40<00:00,  2.90it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:10<00:00,  2.09it/s]\n","                   all        688       4788      0.855      0.835      0.904      0.641\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       2/10      4.31G      1.026     0.6724      1.084          8        640: 100% 292/292 [01:38<00:00,  2.97it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:06<00:00,  3.53it/s]\n","                   all        688       4788      0.896      0.863      0.926       0.64\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       3/10      4.35G      1.005     0.6342      1.075          6        640: 100% 292/292 [01:36<00:00,  3.02it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:09<00:00,  2.39it/s]\n","                   all        688       4788      0.908      0.879      0.937      0.679\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       4/10      4.31G     0.9791     0.5927      1.064         10        640: 100% 292/292 [01:38<00:00,  2.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:06<00:00,  3.51it/s]\n","                   all        688       4788      0.921      0.903      0.945      0.698\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       5/10      4.32G     0.9421     0.5472      1.043          7        640: 100% 292/292 [01:38<00:00,  2.97it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:09<00:00,  2.35it/s]\n","                   all        688       4788      0.944      0.911      0.965      0.728\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       6/10      4.33G     0.9179     0.5134      1.032          7        640: 100% 292/292 [01:37<00:00,  3.01it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:06<00:00,  3.53it/s]\n","                   all        688       4788      0.922       0.91      0.954      0.709\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       7/10      4.27G     0.8932     0.4873      1.019          5        640: 100% 292/292 [01:37<00:00,  2.99it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:08<00:00,  2.56it/s]\n","                   all        688       4788      0.938      0.931      0.969      0.743\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       8/10      4.32G     0.8713      0.463      1.009          9        640: 100% 292/292 [01:39<00:00,  2.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:06<00:00,  3.38it/s]\n","                   all        688       4788      0.942      0.928      0.967      0.744\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       9/10      4.27G     0.8435     0.4361     0.9939         11        640: 100% 292/292 [01:37<00:00,  3.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:08<00:00,  2.56it/s]\n","                   all        688       4788      0.947      0.937      0.972      0.754\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      10/10      4.32G     0.8135     0.4084     0.9828          5        640: 100% 292/292 [01:37<00:00,  3.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:06<00:00,  3.38it/s]\n","                   all        688       4788      0.952       0.94      0.974      0.763\n","\n","10 epochs completed in 0.298 hours.\n","Optimizer stripped from runs/detect/train/weights/last.pt, 18.5MB\n","Optimizer stripped from runs/detect/train/weights/best.pt, 18.5MB\n","\n","Validating runs/detect/train/weights/best.pt...\n","Ultralytics 8.3.37 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 22/22 [00:11<00:00,  1.84it/s]\n","                   all        688       4788      0.953       0.94      0.974      0.763\n","Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 2.4ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train\u001b[0m\n","💡 Learn more at https://docs.ultralytics.com/modes/train\n"]}]},{"cell_type":"code","source":["!cp -r /content/yolov5/runs /content/drive/MyDrive/ColabNotebooks/aps360_dummyResult11-25"],"metadata":{"id":"p8wt7NnO9U03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!yolo task=detect mode=predict model=/content/best-4.pt source=/content/testingFridge2.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TL9EuF6cvK22","executionInfo":{"status":"ok","timestamp":1732679863131,"user_tz":300,"elapsed":7628,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"e34a33a2-8ea4-4fa6-82a3-3246c48da9fd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/testingFridge2.jpg: 640x480 12 dummys, 48.2ms\n","Speed: 3.7ms preprocess, 48.2ms inference, 685.2ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict3\u001b[0m\n","1 label saved to /content/results/predict3/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n"]}]},{"cell_type":"code","source":["!cp -r /content/results /content/drive/MyDrive/ColabNotebooks/aps360_dummyresult_11_26/withMilk"],"metadata":{"id":"mbVz-J9Ojr7Y","executionInfo":{"status":"ok","timestamp":1732679885556,"user_tz":300,"elapsed":1128,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!yolo task=detect mode=predict model=/content/best-4.pt source=/content/10.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/11.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/12.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/2.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/3.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/4.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/5.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/6.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/8.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True\n","!yolo task=detect mode=predict model=/content/best-4.pt source=/content/9.jpg  save=True project=/content/results plots=True save_txt=True save_crop=True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuW2jI17HZHc","executionInfo":{"status":"ok","timestamp":1732669089432,"user_tz":300,"elapsed":62105,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"4edf9f17-5953-45ad-cd11-2d33777bf8f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/10.jpg: 640x384 13 dummys, 46.5ms\n","Speed: 2.9ms preprocess, 46.5ms inference, 830.0ms postprocess per image at shape (1, 3, 640, 384)\n","Results saved to \u001b[1m/content/results/predict3\u001b[0m\n","1 label saved to /content/results/predict3/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/11.jpg: 640x480 6 dummys, 40.8ms\n","Speed: 4.4ms preprocess, 40.8ms inference, 593.4ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict4\u001b[0m\n","1 label saved to /content/results/predict4/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/12.jpg: 640x480 10 dummys, 38.2ms\n","Speed: 3.3ms preprocess, 38.2ms inference, 561.2ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict5\u001b[0m\n","1 label saved to /content/results/predict5/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/2.jpg: 640x480 10 dummys, 41.0ms\n","Speed: 3.8ms preprocess, 41.0ms inference, 620.6ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict6\u001b[0m\n","1 label saved to /content/results/predict6/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/3.jpg: 640x480 6 dummys, 46.9ms\n","Speed: 3.5ms preprocess, 46.9ms inference, 578.0ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict7\u001b[0m\n","1 label saved to /content/results/predict7/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/4.jpg: 640x480 11 dummys, 63.4ms\n","Speed: 4.9ms preprocess, 63.4ms inference, 598.2ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict8\u001b[0m\n","1 label saved to /content/results/predict8/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/5.jpg: 640x480 12 dummys, 48.0ms\n","Speed: 3.4ms preprocess, 48.0ms inference, 588.1ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict9\u001b[0m\n","1 label saved to /content/results/predict9/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/6.jpg: 640x416 8 dummys, 64.7ms\n","Speed: 3.5ms preprocess, 64.7ms inference, 874.5ms postprocess per image at shape (1, 3, 640, 416)\n","Results saved to \u001b[1m/content/results/predict10\u001b[0m\n","1 label saved to /content/results/predict10/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/8.jpg: 640x480 10 dummys, 39.8ms\n","Speed: 3.2ms preprocess, 39.8ms inference, 628.8ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict11\u001b[0m\n","1 label saved to /content/results/predict11/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n","Ultralytics 8.3.38 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n","\n","image 1/1 /content/9.jpg: 640x480 6 dummys, 37.3ms\n","Speed: 2.7ms preprocess, 37.3ms inference, 528.5ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1m/content/results/predict12\u001b[0m\n","1 label saved to /content/results/predict12/labels\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n"]}]},{"cell_type":"code","source":["!cp -r /content/results /content/drive/MyDrive/ColabNotebooks/aps360_dummyresult_11_26"],"metadata":{"id":"dqP-j1ikyUg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/best.pt /content/drive/MyDrive/ColabNotebooks/aps360_dummyResult"],"metadata":{"id":"6270PlrayqJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/yolov5/runs /content/drive/MyDrive/ColabNotebooks/aps360_dummyResult"],"metadata":{"id":"DY5Q5tEny2uE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from models.common import DetectMultiBackend\n","import torch\n","\n","# Set the path to your weights file\n","weights_path = '/content/best.pt'\n","\n","# Set device (GPU if available, otherwise CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the YOLOv5 model\n","model = DetectMultiBackend(weights_path, device=device)\n","\n","# Model structure (optional)\n","print(model.model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"-nd-XIQmZ2wD","executionInfo":{"status":"ok","timestamp":1732506081862,"user_tz":300,"elapsed":1227,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"e153e3ad-c471-429d-8c6f-574d24497173"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["YOLOv5s summary (fused): 193 layers, 9,137,465 parameters, 0 gradients, 24.0 GFLOPs\n","DetectionModel(\n","  (model): Sequential(\n","    (0): Conv(\n","      (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n","      (act): SiLU(inplace=True)\n","    )\n","    (1): Conv(\n","      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (2): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (3): Conv(\n","      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (4): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (5): Conv(\n","      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (6): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","        (2): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (7): Conv(\n","      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (8): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (9): SPPF(\n","      (cv1): Conv(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","    )\n","    (10): Conv(\n","      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (11): Upsample(scale_factor=2.0, mode='nearest')\n","    (12): Concat()\n","    (13): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (14): Conv(\n","      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (15): Upsample(scale_factor=2.0, mode='nearest')\n","    (16): Concat()\n","    (17): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (18): Conv(\n","      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (19): Concat()\n","    (20): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (21): Conv(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (act): SiLU(inplace=True)\n","    )\n","    (22): Concat()\n","    (23): C3(\n","      (cv1): Conv(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv2): Conv(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (cv3): Conv(\n","        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (m): Sequential(\n","        (0): Bottleneck(\n","          (cv1): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (24): Detect(\n","      (cv2): ModuleList(\n","        (0): Sequential(\n","          (0): Conv(\n","            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (1): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Sequential(\n","          (0): Conv(\n","            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (1): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Sequential(\n","          (0): Conv(\n","            (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (1): Conv(\n","            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (cv3): ModuleList(\n","        (0): Sequential(\n","          (0): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (2): Conv2d(128, 67, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Sequential(\n","          (0): Conv(\n","            (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (2): Conv2d(128, 67, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Sequential(\n","          (0): Conv(\n","            (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (1): Conv(\n","            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (2): Conv2d(128, 67, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (dfl): DFL(\n","        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","  )\n",")\n"]}]},{"cell_type":"code","source":["\n","# Ensure model is in evaluation mode\n","model.eval()\n","\n","# Check the model structure (optional)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qCRzlMqMYsLz","executionInfo":{"status":"ok","timestamp":1732506144811,"user_tz":300,"elapsed":176,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"5a1c1d2b-d6c6-442b-bda1-aaf4e62c678d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DetectMultiBackend(\n","  (model): DetectionModel(\n","    (model): Sequential(\n","      (0): Conv(\n","        (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n","        (act): SiLU(inplace=True)\n","      )\n","      (1): Conv(\n","        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (2): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (3): Conv(\n","        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (4): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","          (1): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (5): Conv(\n","        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (6): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","          (1): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","          (2): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (7): Conv(\n","        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (8): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (9): SPPF(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","      )\n","      (10): Conv(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (11): Upsample(scale_factor=2.0, mode='nearest')\n","      (12): Concat()\n","      (13): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (14): Conv(\n","        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (15): Upsample(scale_factor=2.0, mode='nearest')\n","      (16): Concat()\n","      (17): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (18): Conv(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (19): Concat()\n","      (20): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (21): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (act): SiLU(inplace=True)\n","      )\n","      (22): Concat()\n","      (23): C3(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv3): Conv(\n","          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): Sequential(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (24): Detect(\n","        (cv2): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (cv3): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 67, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 67, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 67, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (dfl): DFL(\n","          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","  )\n",")\n"]}]},{"cell_type":"code","source":["%cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VM8TzPrrbpap","executionInfo":{"status":"ok","timestamp":1732506479811,"user_tz":300,"elapsed":121,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"a671e6cd-5ab9-4ff7-90bc-dea968fe20fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["%mkdir croppedImages"],"metadata":{"id":"4Hx8yV1ibwJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/yolov5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFRxhB2Qd3Hb","executionInfo":{"status":"ok","timestamp":1732507049571,"user_tz":300,"elapsed":7,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"d9c9edfc-a647-4c42-821c-10e73b7197e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n"]}]},{"cell_type":"code","source":["\n","from utils.general import non_max_suppression, scale_boxes\n","from utils.augmentations import letterbox\n","\n","\n","stride, names, pt = model.stride, model.names, model.pt\n","\n","# Function to preprocess and resize an image to fit the YOLOv5 input\n","def preprocess_image(img_cv2, stride=32):\n","    # Resize and pad the image to be a multiple of the stride\n","    img = letterbox(img_cv2, stride=stride, auto=True)[0]\n","\n","    # Convert BGR to RGB, transpose dimensions, and normalize (0-255 -> 0-1)\n","    img = img[:, :, ::-1].copy()  # Convert BGR to RGB and remove negative strides\n","    img = img.transpose(2, 0, 1)  # Convert HWC to CHW\n","    img = torch.from_numpy(img).float() / 255.0  # Normalize to [0, 1]\n","    img = img.unsqueeze(0)  # Add batch dimension\n","    return img\n","\n","# Function to process an image and save cropped images of bounding boxes\n","def process_image(image_path, output_dir):\n","    # Load image\n","    img_cv2 = cv2.imread(image_path)  # OpenCV reads in BGR format\n","    img_tensor = preprocess_image(img_cv2, stride)  # Preprocess the image\n","\n","    # Run inference\n","    pred = model(img_tensor, augment=False, visualize=False)  # Raw predictions\n","\n","    # Apply non-max suppression to filter predictions\n","    pred = non_max_suppression(pred, conf_thres=0.5, iou_thres=0.45, classes=None, agnostic=False)\n","\n","    for i, det in enumerate(pred):  # Iterate over detections\n","        if len(det):\n","            # Rescale boxes to original image dimensions\n","            det[:, :4] = scale_boxes(img_tensor.shape[2:], det[:, :4], img_cv2.shape).round()\n","\n","            # Iterate over each detection\n","            for j, (*xyxy, conf, cls) in enumerate(det):\n","                xmin, ymin, xmax, ymax = map(int, xyxy)\n","                cropped_img = img_cv2[ymin:ymax, xmin:xmax]  # Crop bounding box\n","\n","                # Save the cropped image\n","                save_path = os.path.join(output_dir, f\"{Path(image_path).stem}_{j}.jpg\")\n","                cv2.imwrite(save_path, cropped_img)\n","\n","# Directory paths\n","image_dir = '/content/Combined-dataset-5/train/images'  # Replace with the directory containing your images\n","output_dir = '/content/croppedImages'  # Replace with the output directory for cropped images\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Process all images in the directory\n","for img_file in os.listdir(image_dir):\n","    if img_file.endswith(('.jpg', '.png', '.jpeg')):\n","        process_image(os.path.join(image_dir, img_file), output_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"tJkmGVpKdOcM","executionInfo":{"status":"error","timestamp":1732507380880,"user_tz":300,"elapsed":76681,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"7e0a8ee2-7b96-47ca-f444-5b0b2fd082cc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-4102ca969393>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-4102ca969393>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image_path, output_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raw predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Apply non-max suppression to filter predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TorchScript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/head.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Training path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;34m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["\n","from pathlib import Path\n","# Set confidence threshold very low to ensure all boxes are drawn\n","model.model.conf = 0.01  # Confidence threshold\n","model.eval()  # Set model to evaluation mode\n","\n","# Function to process an image and save cropped images of bounding boxes\n","def process_image(image_path, output_dir):\n","    # Load and preprocess image\n","    img_cv2 = cv2.imread(image_path)  # OpenCV reads in BGR format\n","    img = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n","\n","    # Run inference\n","    results = model(img)  # Perform inference using the loaded YOLOv5 model\n","\n","    # Get detections (xmin, ymin, xmax, ymax, confidence, class)\n","    detections = results.xyxy[0].cpu().numpy()\n","\n","    for i, detection in enumerate(detections):\n","        xmin, ymin, xmax, ymax, confidence, cls = detection.tolist()\n","        cropped_img = img_cv2[int(ymin):int(ymax), int(xmin):int(xmax)]  # Crop bounding box\n","\n","        # Save the cropped image\n","        save_path = os.path.join(output_dir, f\"{Path(image_path).stem}_{i}.jpg\")\n","        cv2.imwrite(save_path, cropped_img)\n","\n","# Directory paths\n","image_dir = '/content/Combined-dataset-5/train/images'  # Replace with the directory containing your images\n","output_dir = '/content/croppedImages'  # Replace with the output directory for cropped images\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Process all images in the directory\n","for img_file in os.listdir(image_dir):\n","    if img_file.endswith(('.jpg', '.png', '.jpeg')):\n","        process_image(os.path.join(image_dir, img_file), output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"lalso2S6bW4g","executionInfo":{"status":"error","timestamp":1732506656520,"user_tz":300,"elapsed":265,"user":{"displayName":"Zha Yixin","userId":"06538431514083436016"}},"outputId":"ab5073ba-030a-43cd-e614-8ed2c6f7bcaa"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 3)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-0a899561d512>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-0a899561d512>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image_path, output_dir)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Perform inference using the loaded YOLOv5 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Get detections (xmin, ymin, xmax, ymax, confidence, class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;34m\"\"\"Performs YOLOv5 inference on input images with options for augmentation and visualization.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;31m# batch, channel, height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# to FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-72JPm_UVGMm"},"outputs":[],"source":["\n","\n","# Load YOLO model\n","model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')  # Replace with your trained YOLO model\n","model.conf = 0.01  # Set confidence threshold very low to ensure all boxes are drawn\n","\n","# Function to process an image and save cropped images of bounding boxes\n","def process_image(image_path, output_dir):\n","    # Load image\n","    img = Image.open(image_path)\n","    results = model(img)\n","\n","    # Get detections\n","    detections = results.xyxy[0]  # [xmin, ymin, xmax, ymax, confidence, class]\n","    img_cv2 = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","\n","    for i, detection in enumerate(detections):\n","        xmin, ymin, xmax, ymax, confidence, cls = detection.tolist()\n","        cropped_img = img_cv2[int(ymin):int(ymax), int(xmin):int(xmax)]  # Crop bounding box\n","        save_path = os.path.join(output_dir, f\"{os.path.basename(image_path).split('.')[0]}_{i}.jpg\")\n","        cv2.imwrite(save_path, cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR))\n","\n","# Directory paths\n","image_dir = 'path/to/fridge/images'\n","output_dir = 'path/to/cropped_images'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Process all images in directory\n","for img_file in os.listdir(image_dir):\n","    if img_file.endswith(('.jpg', '.png')):\n","        process_image(os.path.join(image_dir, img_file), output_dir)\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torchvision import datasets, models\n","from torch.utils.data import DataLoader\n","\n","# Define CNN model\n","class FoodClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(FoodClassifier, self).__init__()\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define dataset and loader\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","dataset = datasets.ImageFolder('path/to/cropped_images', transform=transform)\n","data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","# Initialize and train model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_classes = len(dataset.classes)  # Assuming your cropped dataset has folders for each class\n","model = FoodClassifier(num_classes).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","epochs = 10\n","for epoch in range(epochs):\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"],"metadata":{"id":"mazpw_xbVRkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","\n","yolo_data_dir = 'path/to/yolo/dataset'\n","output_dir = 'path/to/cropped_dataset'\n","\n","os.makedirs(output_dir, exist_ok=True)\n","\n","for img_file in os.listdir(yolo_data_dir):\n","    if img_file.endswith('.jpg'):  # Assuming images are .jpg\n","        txt_file = img_file.replace('.jpg', '.txt')\n","        img_path = os.path.join(yolo_data_dir, img_file)\n","        txt_path = os.path.join(yolo_data_dir, txt_file)\n","\n","        if not os.path.exists(txt_path):\n","            continue\n","\n","        img = cv2.imread(img_path)\n","        with open(txt_path, 'r') as f:\n","            lines = f.readlines()\n","            for i, line in enumerate(lines):\n","                class_idx, x_center, y_center, width, height = map(float, line.split())\n","                h, w, _ = img.shape\n","                xmin = int((x_center - width / 2) * w)\n","                ymin = int((y_center - height / 2) * h)\n","                xmax = int((x_center + width / 2) * w)\n","                ymax = int((y_center + height / 2) * h)\n","\n","                cropped_img = img[ymin:ymax, xmin:xmax]\n","                class_dir = os.path.join(output_dir, str(int(class_idx)))\n","                os.makedirs(class_dir, exist_ok=True)\n","                save_path = os.path.join(class_dir, f\"{img_file.split('.')[0]}_{i}.jpg\")\n","                cv2.imwrite(save_path, cropped_img)\n"],"metadata":{"id":"SR49cpsdVZEZ"},"execution_count":null,"outputs":[]}]}