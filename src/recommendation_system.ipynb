{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The file is based on the Google colab. The file focus on creating a recommendation system based on an detected food vector.\n",
        "## The food recipe dataset is from the Kaggle. Food Ingredients and Recipes Dataset with Images: https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images"
      ],
      "metadata": {
        "id": "7VUVN8hVXhfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **The Baseline Model: cosine similarity**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-_2X2Q1VLlFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dataset** **download**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cwFSgwFVX3aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pes12017000148/food-ingredients-and-recipe-dataset-with-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ySUQDJbgc6",
        "outputId": "f5546e6e-d3e7-49a2-dfa6-9f5a50449462"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/pes12017000148/food-ingredients-and-recipe-dataset-with-images?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 206M/206M [00:01<00:00, 125MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Dataset** **loading**"
      ],
      "metadata": {
        "id": "S6nyXN9EYqFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "### data loading\n",
        "data_path = \"/root/.cache/kagglehub/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images/versions/1\"\n",
        "csv_file = os.path.join(data_path, \"Food Ingredients and Recipe Dataset with Image Name Mapping.csv\")\n",
        "df = pd.read_csv(csv_file)"
      ],
      "metadata": {
        "id": "KL5WKC7rxHIR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Cleaned_Ingredients[5]:\")\n",
        "cleaned_ingredients = eval(df['Cleaned_Ingredients'].iloc[2])\n",
        "Length_limit = 4\n",
        "for i in range(0, len(cleaned_ingredients), Length_limit):\n",
        "    print(cleaned_ingredients[i:i + Length_limit])"
      ],
      "metadata": {
        "id": "aC4LkWXAxmqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data** **cleaning**"
      ],
      "metadata": {
        "id": "5sHrIV4faNJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### data cleaning\n",
        "df = df.dropna(subset=['Cleaned_Ingredients', 'Title', 'Instructions'])\n",
        "df = df.drop_duplicates(subset=['Title'])"
      ],
      "metadata": {
        "id": "UMujT1xTaKVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Main** **Body Part**"
      ],
      "metadata": {
        "id": "gKznqhlaaeiB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmSEW2jOD3zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4eb9c16-1e91-471f-f57f-942b906cf391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Recommended Recipes based on the food detection in fridge:\n",
            "Top1. Title: Spicy Shrimp and Vegetable Stir-Fry, Cosine Similarity: 0.3508\n",
            "\n",
            "Ingredients:\n",
            "['1/4 cup low-sodium soy sauce', '1/4 cup sake', '2 tablespoons sugar', '1 tablespoon dark (toasted) sesame oil', '1 tablespoon chopped garlic', '1 tablespoon\n",
            "finely chopped or grated ginger', '1 cup large-diced red bell pepper', '1 cup large-diced green bell pepper', '1 cup large-diced onion', '1 cup cubed cabbage',\n",
            "'1 cup sliced carrot', '1/2 teaspoon red pepper flakes', '24 large shrimp', 'shelled and deveined']\n",
            "\n",
            "Top2. Title: 3-Ingredient Peanut Butter Cookies, Cosine Similarity: 0.3162\n",
            "\n",
            "Ingredients:\n",
            "['1 large egg', '1 cup creamy peanut butter', '1 cup sugar', 'Flaky sea salt (optional)']\n",
            "\n",
            "Top3. Title: Stuffed Meatloaf, Cosine Similarity: 0.3162\n",
            "\n",
            "Ingredients:\n",
            "['1 large egg', '1/4 cup finely chopped onion', '1/4 cup seasoned breadcrumbs', '1 tablespoon ketchup', '1/2 teaspoon Worcestershire sauce', 'Salt', 'Freshly\n",
            "ground pepper', '1/2 pound ground beef', '4 small balls fresh mozzarella', '1 (8-ounce) can tomato sauce']\n",
            "\n",
            "Top4. Title: Easter Bread Dolls (Pupi or Titola), Cosine Similarity: 0.3162\n",
            "\n",
            "Ingredients:\n",
            "['1 recipe pinza dough , prepared through first rise and divided into 3 balls', '3 dyed Easter eggs', '1 large egg', '2 tablespoons sugar']\n",
            "\n",
            "Top5. Title: Chocolate-Dipped Orange Peel, Cosine Similarity: 0.2828\n",
            "\n",
            "Ingredients:\n",
            "['5 oranges', 'Water', 'Salt', '2 1/3 cups sugar', '10 1/2 oz dark chocolate (70% cocoa)']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract only the last word from the sentences in the list, ignoring the others\n",
        "def ingredient_standarization(ingredient):\n",
        "    words = ingredient.split()\n",
        "    if len(words) > 1:\n",
        "        return words[-1]\n",
        "    elif words:\n",
        "        return words[0]\n",
        "    return \"\"\n",
        "\n",
        "### Standardize the cleaned_ingredients column\n",
        "ingre_std_list = []\n",
        "for ingredients in df['Cleaned_Ingredients']:\n",
        "    ingre_std = []\n",
        "    for ingredient in eval(ingredients):  # convert a string list into a Python list\n",
        "        ingre_std.append(ingredient_standarization(ingredient))  # use function to standardize every ingredient\n",
        "    ingre_std_list.append(ingre_std)\n",
        "\n",
        "df['Ingredients_list'] = ingre_std_list\n",
        "\n",
        "### filter the null string\n",
        "ingre_filter_list = []\n",
        "for ingredients in df['Ingredients_list']:\n",
        "    ingre_filtered = []\n",
        "    for ingredient in ingredients:\n",
        "        ### check if the string is not ' '\n",
        "        if ingredient.strip():\n",
        "            ingre_filtered.append(ingredient)\n",
        "    ingre_filter_list.append(ingre_filtered)\n",
        "\n",
        "df['Ingredients_list'] = ingre_filter_list\n",
        "\n",
        "### collect all ingredients from the ingredients list\n",
        "ingredient_list_total = set()\n",
        "for ingredients in df['Ingredients_list']:\n",
        "    for ingredient in ingredients:\n",
        "        if ingredient.strip():\n",
        "            ingredient_list_total.add(ingredient)\n",
        "\n",
        "\n",
        "### use the ingredients as the labels of our MLB\n",
        "mlb = MultiLabelBinarizer(classes=list(ingredient_list_total))\n",
        "ingredient_vectors = mlb.fit_transform(df['Ingredients_list'])\n",
        "\n",
        "# input vector from YOLO\n",
        "YOLO_detected_ingredients = ['beef', 'corn', 'egg','cucumber','spinach','sugar','cabbage','garlic','Salt','cucumber','carrot']\n",
        "\n",
        "# convert YOLO vector from char list to one-hot encoding\n",
        "YOLO_vector_one_hot = mlb.transform([YOLO_detected_ingredients])[0]\n",
        "\n",
        "# calculate the cosine similarity\n",
        "cos_sim = cosine_similarity([YOLO_vector_one_hot], ingredient_vectors).flatten()\n",
        "\n",
        "## exclude the recipe with cosine similarity 0\n",
        "non_zero_indices = cos_sim > 0\n",
        "filtered_cos_sim = cos_sim[non_zero_indices]\n",
        "filtered_indices = non_zero_indices.nonzero()[0]\n",
        "\n",
        "# sort recipes without 0 cosine similarity and get the top 5 recipes\n",
        "top_5_recipe_idx = filtered_cos_sim.argsort()[-5:][::-1]\n",
        "top_5_recipe_idx_filtered = filtered_indices[top_5_recipe_idx]\n",
        "top_5_recipes = df.iloc[top_5_recipe_idx_filtered]\n",
        "\n",
        "### output adjust function, adjust the output display\n",
        "def output_adjust(text, line_length=160):\n",
        "    words = text.split()\n",
        "    text_adjust = \"\"\n",
        "    line = \"\"\n",
        "    for word in words:\n",
        "        if len(line) + len(word) + 1 > line_length:\n",
        "            text_adjust += line + \"\\n\"\n",
        "            line = word\n",
        "        else:\n",
        "            if line:\n",
        "                line += \" \" + word\n",
        "            else:\n",
        "                line = word\n",
        "    text_adjust += line\n",
        "    return text_adjust\n",
        "\n",
        "## print the output\n",
        "print(\"Top 5 Recommended Recipes based on the food detection in fridge:\")\n",
        "for i, index in enumerate(top_5_recipe_idx_filtered, start=1):\n",
        "    title = df.iloc[index]['Title']\n",
        "    instructions = df.iloc[index]['Instructions']\n",
        "    ingredients = df.iloc[index]['Cleaned_Ingredients']\n",
        "    similarity = cos_sim[index]\n",
        "    formatted_instructions = output_adjust(instructions)\n",
        "    ingredients = output_adjust(ingredients)\n",
        "    print(f\"Top{i}. Title: {title}, Cosine Similarity: {similarity:.4f}\\n\")\n",
        "    print(f\"Ingredients:\\n{ingredients}\\n\")\n",
        "    # print(f\"Instructions:\\n{formatted_instructions}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Cleaned ingredient list[5] using baseline model:\")\n",
        "ingredients = df['Ingredients_list'].iloc[5]\n",
        "line_length = 8\n",
        "for i in range(0, len(ingredients), line_length):\n",
        "    print(ingredients[i:i + line_length])"
      ],
      "metadata": {
        "id": "bJDzNKB4D5fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bb408d-8be7-4e71-9656-ef41ef0abdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cleaned ingredient list[5] using baseline model:\n",
            "['bags', 'tequila', 'juice', 'nectar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Primary model: NER-enhanced recommendation system**"
      ],
      "metadata": {
        "id": "t09uzdm_MECH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Primary model use Named Entity Network to perform the food extraction process. The NER network the team use is Spacy's en-core-web-sm model.\n"
      ],
      "metadata": {
        "id": "mKOAApDmaon3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import time\n",
        "\n",
        "### GPU statement\n",
        "try:\n",
        "    spacy.require_gpu()\n",
        "    print(\"GPU is available in the training.\")\n",
        "except:\n",
        "    print(\"GPU is not available, using CPU instead.\")\n",
        "\n",
        "# load the Spacy model\n",
        "NER_SPACY = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# the list of words in noun but not food ingredients\n",
        "words_not_food_in_noun = {\n",
        "              # \"cup\", \"teaspoon\", \"tablespoons\", \"cups\", \"ounce\", \"ounces\", \"pound\", \"pounds\", \"tsp\",\n",
        "              # \"tbsp\", \"medium\", \"stick\", \"slices\", \"extract\", \"oz\", \"temperature\", \"room\", \"pieces\",\n",
        "              # \"purpose\", \"total\", \"g\", \"ml\", \"lengthwise\", \"crosswise\", \"pinch\", \"½\", \"¼\", \"¾\",\n",
        "              # \"package\", \"sprigs\", \"sticks\", \"halves\", \"inch\", \"bunch\", \"bunches\", \"parts\", \"quality\",\n",
        "              # \"batch\", \"thermometer\", \"slice\", \"grind\", \"attachment\", \"wheel\", \"superfine\", \"quarts\",\n",
        "              # \"baking\", \"ribbons\", \"dash\", \"layers\", \"cube\", \"skewers\", \"sheets\", \"moons\", \"round\",\n",
        "              # \"rind\", \"top\", \"bottom\", \"height\", \"width\", \"diameter\", \"bias\", \"flameproof\",\n",
        "              # \"rolling\", \"pin\", \"center\", \"frying\", \"mill\", \"maker\", \"heaping\", \"¼\", \"sticks\",\n",
        "              # \"tops\", \"ends\", \"weights\", \"kitchen\", \"tool\", \"processor\", \"ramekins\", \"thickness\",\n",
        "              # \"molds\", \"sheet\", \"paddle\", \"strip\", \"segments\", \"blocks\", \"form\", \"stand\",\n",
        "              # \"lengths\", \"handle\", \"springform\", \"sheets\", \"platter\", \"knives\", \"blender\", \"mandoline\",\n",
        "              # \"skillet\", \"strainer\", \"cast\", \"iron\", \"jar\", \"cans\", \"quart\", \"pint\", \"ruler\",\n",
        "              # \"bottle\", \"mortar\", \"pestle\", \"sticks\", \"notes\", \"step\", \"layer\", \"loaf\",\n",
        "              # \"pan\", \"batch\", \"inch\",  \"foil\", \"container\", \"tins\", \"shells\",\n",
        "              # \"mold\", \"serving\", \"blender\", \"processor\", \"attachment\", \"form\", \"box\",\n",
        "              # \"bowl\", \"spatula\", \"mixer\", \"wire\", \"frame\", \"plate\", \"sheet\",\n",
        "              # \"nut\", \"oven\", \"tray\", \"cutter\", \"grater\", \"pot\", \"wok\", \"dough\", \"log\", \"weight\",\n",
        "              # \"temperature\", \"reading\", \"minutes\", \"hours\", \"tables\", \"range\", \"measuring\", \"bag\",\n",
        "              # \"envelope\", \"mat\", \"sprinkling\", \"tin\", \"edges\", \"ball\", \"circle\", \"seal\", \"quart\",\n",
        "              # \"syrup\", \"slab\", \"bars\", \"kitchen\", \"molds\", \"ribs\", \"base\", \"lump\", \"skewer\",\n",
        "              # \"thickness\", \"degree\", \"ladle\", \"grains\", \"milliliters\", \"gallon\", \"basting\", \"stem\",\n",
        "              # \"tray\", \"handful\", \"frames\", \"loops\", \"bag\", \"label\"\n",
        "              }\n",
        "\n",
        "# food ingredient word extracting\n",
        "def food_word_extracting(ingredients_list, batch_size=32):\n",
        "    food_extracted_output = []\n",
        "    total_batch = (len(ingredients_list) +batch_size-1) // batch_size  ### calculate the total batch sizes\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx in range(0, len(ingredients_list), batch_size):\n",
        "        ### seperate the total batch into [0,32],[32,64]....\n",
        "        batch = ingredients_list[batch_idx:batch_idx + batch_size]\n",
        "        batch_docs = list(NER_SPACY.pipe(batch))\n",
        "        batch_output = []\n",
        "\n",
        "        ### the important steps, we ultilize the Spacy noun words extracting ability to extract nouns and propns from the sentences.\n",
        "        ### one slice of the cake ----> \"cake\" will be extracted\n",
        "        for doc in batch_docs:\n",
        "            food_ing_words = []\n",
        "            for token in doc:\n",
        "                ### if the token is in the noun and propn and not in the word_not_food_in_noun list\n",
        "                if token.pos_ in {\"NOUN\", \"PROPN\"} and token.text.lower() not in words_not_food_in_noun:\n",
        "                    food_ing_words.append(token.text)\n",
        "            batch_output.append(food_ing_words)\n",
        "        food_extracted_output.extend(batch_output)\n",
        "\n",
        "        # print the logs\n",
        "        time_cost = time.time() - start_time\n",
        "        if (batch_idx // batch_size+1) % 100 == 0:  # print log every 10 batchs\n",
        "            print(f\"Processed batch {batch_idx // batch_size+1}/{total_batch}. Time cost is : {time_cost:.2f} seconds\")\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Word extracting process using NER completed in {total_time:.2f} seconds.\")\n",
        "    return food_extracted_output\n",
        "\n",
        "# data loading\n",
        "data_path = \"/root/.cache/kagglehub/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images/versions/1\"\n",
        "csv_file = os.path.join(data_path, \"Food Ingredients and Recipe Dataset with Image Name Mapping.csv\")\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# data cleaning\n",
        "df = df.dropna(subset=['Cleaned_Ingredients', 'Title', 'Instructions'])\n",
        "df = df.drop_duplicates(subset=['Title'])\n",
        "\n",
        "### Standardize the cleaned_ingredients column (the improving step: we use the Spacy ner network to do the word extracting)\n",
        "ingre_cleaned_merge = []\n",
        "for cleaned_ingredients in df['Cleaned_Ingredients']:\n",
        "    # use eval function and ' ' to merge every word into a sentence with the form of python list\n",
        "    combined_text = \" \".join(eval(cleaned_ingredients))\n",
        "    ingre_cleaned_merge.append(combined_text)\n",
        "\n",
        "# use food_extracting function to extract the food words in the merged sentences\n",
        "ingredients_list = food_word_extracting(ingre_cleaned_merge)\n",
        "df['Ingredients_list'] = ingredients_list\n",
        "\n",
        "# input vector from YOLO output\n",
        "YOLO_detected_ingredients = ['beef', 'corn', 'egg', 'cucumber', 'spinach', 'sugar', 'cabbage', 'garlic', 'Salt', 'cucumber', 'carrot']\n",
        "\n",
        "# collect all ingredients from the ingredients list\n",
        "ingredient_list_total = set()\n",
        "for ingredients in df['Ingredients_list']:\n",
        "    for ingredient in ingredients:\n",
        "        if ingredient.strip():\n",
        "            ingredient_list_total.add(ingredient)\n",
        "\n",
        "# merge the YOLO food ingredient into the ingredient_list_total\n",
        "ingredient_list_total.update(YOLO_detected_ingredients)\n",
        "\n",
        "### use the ingredients as the labels of our MLB\n",
        "mlb = MultiLabelBinarizer(classes=list(ingredient_list_total))\n",
        "ingredient_vectors = mlb.fit_transform(df['Ingredients_list'])\n",
        "\n",
        "# convert YOLO vector into the one hot encoding\n",
        "YOLO_vector_one_hot = mlb.transform([YOLO_detected_ingredients])[0]\n",
        "\n",
        "# calculate the cosine similarity\n",
        "cos_sim = cosine_similarity([YOLO_vector_one_hot], ingredient_vectors).flatten()\n",
        "\n",
        "## exclude the recipe with cosine similarity 0\n",
        "non_zero_indices = cos_sim > 0\n",
        "filtered_cos_sim = cos_sim[non_zero_indices]\n",
        "filtered_indices = non_zero_indices.nonzero()[0]\n",
        "\n",
        "# sort recipes without 0 cosine similarity and get the top 5 recipes\n",
        "top_5_recipe_idx = filtered_cos_sim.argsort()[-5:][::-1]\n",
        "top_5_recipe_idx_filtered = filtered_indices[top_5_recipe_idx]\n",
        "top_5_recipes = df.iloc[top_5_recipe_idx_filtered]\n",
        "\n",
        "### output adjust function, adjust the output display\n",
        "def output_adjust(text, line_length=160):\n",
        "    words = text.split()\n",
        "    text_adjust = \"\"\n",
        "    line = \"\"\n",
        "    for word in words:\n",
        "        if len(line) + len(word) + 1 > line_length:\n",
        "            text_adjust += line + \"\\n\"\n",
        "            line = word\n",
        "        else:\n",
        "            if line:\n",
        "                line += \" \" + word\n",
        "            else:\n",
        "                line = word\n",
        "    text_adjust += line\n",
        "    return text_adjust\n",
        "\n",
        "# # print the output\n",
        "# print(\"Top 5 Recommended Recipes based on the food detection in fridge:\")\n",
        "# for i, index in enumerate(top_5_recipe_idx_filtered, start=1):\n",
        "#     title = df.iloc[index]['Title']\n",
        "#     instructions = df.iloc[index]['Instructions']\n",
        "#     ingredients = df.iloc[index]['Cleaned_Ingredients']\n",
        "#     similarity = cos_sim[index]\n",
        "#     formatted_instructions = output_adjust(instructions)\n",
        "#     ingredients = output_adjust(ingredients)\n",
        "#     print(f\"Top{i}. Title: {title}, Cosine Similarity: {similarity:.4f}\\n\")\n",
        "#     print(f\"Ingredients:\\n{ingredients}\\n\")\n",
        "#     print(f\"Instructions:\\n{formatted_instructions}\\n\")\n",
        "\n",
        "# Print the output\n",
        "print(\"Top 5 Recommended Recipes based on the food detection in fridge:\")\n",
        "for i, index in enumerate(top_5_recipe_idx_filtered, start=1):\n",
        "    title = df.iloc[index]['Title']\n",
        "    # use the processed ingredient list with extracted nouns and pronouns\n",
        "    ingredients = df.iloc[index]['Ingredients_list']\n",
        "    # measure the same ingredients\n",
        "    yolo_set = set(YOLO_detected_ingredients)\n",
        "    merged_ingre_set = set(ingredients)\n",
        "    same_ingre = yolo_set.intersection(merged_ingre_set)\n",
        "    same_ingre_string = \", \".join(same_ingre)\n",
        "\n",
        "    print(f\"Top {i}. Title: {title}, Cosine Similarity: {cos_sim[index]:.4f}\\n\")\n",
        "    print(f\"There are {len(same_ingre)} ingredients detected, which are the [{same_ingre_string}].\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VAV1IxVFMDWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44cc0648-4293-4b69-d1ff-902525a1416c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available in the training.\n",
            "Processed batch 100/416. Time cost is : 15.01 seconds\n",
            "Processed batch 200/416. Time cost is : 27.59 seconds\n",
            "Processed batch 300/416. Time cost is : 42.51 seconds\n",
            "Processed batch 400/416. Time cost is : 55.45 seconds\n",
            "Word extracting process using NER completed in 57.07 seconds.\n",
            "Top 5 Recommended Recipes based on the food detection in fridge:\n",
            "Top 1. Title: Bibimbap, Cosine Similarity: 0.3101\n",
            "\n",
            "There are 5 ingredients detected, which are the [beef, corn, carrot, sugar, spinach].\n",
            "\n",
            "Top 2. Title: Slow-Cooked Venison, Cosine Similarity: 0.2936\n",
            "\n",
            "There are 5 ingredients detected, which are the [beef, carrot, garlic, sugar, Salt].\n",
            "\n",
            "Top 3. Title: Spicy Shrimp and Vegetable Stir-Fry, Cosine Similarity: 0.2902\n",
            "\n",
            "There are 4 ingredients detected, which are the [carrot, garlic, sugar, cabbage].\n",
            "\n",
            "Top 4. Title: Vegetable Latkes, Cosine Similarity: 0.2828\n",
            "\n",
            "There are 4 ingredients detected, which are the [carrot, corn, sugar, spinach].\n",
            "\n",
            "Top 5. Title: Avocado Egg-in-a-Hole, Cosine Similarity: 0.2828\n",
            "\n",
            "There are 2 ingredients detected, which are the [egg, Salt].\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the spaCy's effectiveness in extracting the food word terms**"
      ],
      "metadata": {
        "id": "6jyYJj0IY8op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Cleaned ingredient list[5] Using SPACY:\")\n",
        "ingredients = df['Ingredients_list'].iloc[5]\n",
        "line_length = 9\n",
        "for i in range(0, len(ingredients), line_length):\n",
        "    print(ingredients[i:i + line_length])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kElztyTynQ2",
        "outputId": "df2e8134-5742-47a0-afbb-8d070672546f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cleaned ingredient list[5] Using SPACY:\n",
            "['chamomile', 'tea', 'bags', 'reposado', 'tequila', 'lemon', 'juice', 'agave', 'nectar']\n"
          ]
        }
      ]
    }
  ]
}